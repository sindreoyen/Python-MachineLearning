{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Delivery 2: Learning Decision Models\n",
    "#### *by Sindre Øyen*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This paper will present a comprehensive analysis of the Hepatitis C Virus (HCV) dataset from the University of California, Irvine (UCI) Machine Learning Repository [1]. The focus of the paper will be to apply and evaluate various decision modeling techniques - encompassing preprocessing, model construction, and model evaluation. The paper will explore multiple preprocessing strategies, including handling missing values and feature modification, to prepare the dataset for the use in different machine learning models. These models will range from instance-based learning and decision trees to ensemble learning with trees and neural networks. The evaluation of the models will be based on a balanced construction, performance metrics, and yield curves, offering insights into their applicability in healthcare data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To initialize this study, the dataset itself can be loaded from the ICU database as such:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# Loading in the dataset\n",
    "hcv_data = fetch_ucirepo(id=571) \n",
    "  \n",
    "# Separating the features and target\n",
    "X = hcv_data.data.features \n",
    "y = hcv_data.data.targets \n",
    "  \n",
    "def printInfo():\n",
    "    print(hcv_data.metadata)  \n",
    "    print(hcv_data.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2 Preprocessing of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will perform a reasoned construction of various versions of the dataset. These will be possible to differentiate in the later performed work with the data to evaluate performance on different versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ALB</th>\n",
       "      <th>ALP</th>\n",
       "      <th>AST</th>\n",
       "      <th>BIL</th>\n",
       "      <th>CHE</th>\n",
       "      <th>CHOL</th>\n",
       "      <th>CREA</th>\n",
       "      <th>CGT</th>\n",
       "      <th>PROT</th>\n",
       "      <th>ALT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>38.5</td>\n",
       "      <td>52.5</td>\n",
       "      <td>22.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6.93</td>\n",
       "      <td>3.23</td>\n",
       "      <td>106.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>69.0</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>38.5</td>\n",
       "      <td>70.3</td>\n",
       "      <td>24.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>11.17</td>\n",
       "      <td>4.80</td>\n",
       "      <td>74.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>76.5</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>46.9</td>\n",
       "      <td>74.7</td>\n",
       "      <td>52.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.84</td>\n",
       "      <td>5.20</td>\n",
       "      <td>86.0</td>\n",
       "      <td>33.2</td>\n",
       "      <td>79.3</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>43.2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>22.6</td>\n",
       "      <td>18.9</td>\n",
       "      <td>7.33</td>\n",
       "      <td>4.74</td>\n",
       "      <td>80.0</td>\n",
       "      <td>33.8</td>\n",
       "      <td>75.7</td>\n",
       "      <td>30.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>39.2</td>\n",
       "      <td>74.1</td>\n",
       "      <td>24.8</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.15</td>\n",
       "      <td>4.32</td>\n",
       "      <td>76.0</td>\n",
       "      <td>29.9</td>\n",
       "      <td>68.7</td>\n",
       "      <td>32.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>62</td>\n",
       "      <td>f</td>\n",
       "      <td>32.0</td>\n",
       "      <td>416.6</td>\n",
       "      <td>110.3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.57</td>\n",
       "      <td>6.30</td>\n",
       "      <td>55.7</td>\n",
       "      <td>650.9</td>\n",
       "      <td>68.5</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>64</td>\n",
       "      <td>f</td>\n",
       "      <td>24.0</td>\n",
       "      <td>102.8</td>\n",
       "      <td>44.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>3.02</td>\n",
       "      <td>63.0</td>\n",
       "      <td>35.9</td>\n",
       "      <td>71.3</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>64</td>\n",
       "      <td>f</td>\n",
       "      <td>29.0</td>\n",
       "      <td>87.3</td>\n",
       "      <td>99.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.63</td>\n",
       "      <td>66.7</td>\n",
       "      <td>64.2</td>\n",
       "      <td>82.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>46</td>\n",
       "      <td>f</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.56</td>\n",
       "      <td>4.20</td>\n",
       "      <td>52.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>59</td>\n",
       "      <td>f</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.07</td>\n",
       "      <td>5.30</td>\n",
       "      <td>67.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>615 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age Sex   ALB    ALP    AST   BIL    CHE  CHOL   CREA    CGT  PROT    ALT\n",
       "0     32   m  38.5   52.5   22.1   7.5   6.93  3.23  106.0   12.1  69.0    7.7\n",
       "1     32   m  38.5   70.3   24.7   3.9  11.17  4.80   74.0   15.6  76.5   18.0\n",
       "2     32   m  46.9   74.7   52.6   6.1   8.84  5.20   86.0   33.2  79.3   36.2\n",
       "3     32   m  43.2   52.0   22.6  18.9   7.33  4.74   80.0   33.8  75.7   30.6\n",
       "4     32   m  39.2   74.1   24.8   9.6   9.15  4.32   76.0   29.9  68.7   32.6\n",
       "..   ...  ..   ...    ...    ...   ...    ...   ...    ...    ...   ...    ...\n",
       "610   62   f  32.0  416.6  110.3  50.0   5.57  6.30   55.7  650.9  68.5    5.9\n",
       "611   64   f  24.0  102.8   44.4  20.0   1.54  3.02   63.0   35.9  71.3    2.9\n",
       "612   64   f  29.0   87.3   99.0  48.0   1.66  3.63   66.7   64.2  82.0    3.5\n",
       "613   46   f  33.0    NaN   62.0  20.0   3.56  4.20   52.0   50.0  71.0   39.0\n",
       "614   59   f  36.0    NaN   80.0  12.0   9.07  5.30   67.0   34.0  68.0  100.0\n",
       "\n",
       "[615 rows x 12 columns]"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>ALB</th>\n",
       "      <th>ALP</th>\n",
       "      <th>AST</th>\n",
       "      <th>BIL</th>\n",
       "      <th>CHE</th>\n",
       "      <th>CHOL</th>\n",
       "      <th>CREA</th>\n",
       "      <th>CGT</th>\n",
       "      <th>PROT</th>\n",
       "      <th>ALT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>615.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>597.000000</td>\n",
       "      <td>615.000000</td>\n",
       "      <td>615.000000</td>\n",
       "      <td>615.000000</td>\n",
       "      <td>605.000000</td>\n",
       "      <td>615.000000</td>\n",
       "      <td>615.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>47.408130</td>\n",
       "      <td>41.620195</td>\n",
       "      <td>68.283920</td>\n",
       "      <td>34.786341</td>\n",
       "      <td>11.396748</td>\n",
       "      <td>8.196634</td>\n",
       "      <td>5.368099</td>\n",
       "      <td>81.287805</td>\n",
       "      <td>39.533171</td>\n",
       "      <td>72.044137</td>\n",
       "      <td>28.450814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.055105</td>\n",
       "      <td>5.780629</td>\n",
       "      <td>26.028315</td>\n",
       "      <td>33.090690</td>\n",
       "      <td>19.673150</td>\n",
       "      <td>2.205657</td>\n",
       "      <td>1.132728</td>\n",
       "      <td>49.756166</td>\n",
       "      <td>54.661071</td>\n",
       "      <td>5.402636</td>\n",
       "      <td>25.469689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.420000</td>\n",
       "      <td>1.430000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>44.800000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>38.800000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>6.935000</td>\n",
       "      <td>4.610000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>69.300000</td>\n",
       "      <td>16.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>66.200000</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>8.260000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>23.300000</td>\n",
       "      <td>72.200000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>45.200000</td>\n",
       "      <td>80.100000</td>\n",
       "      <td>32.900000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>9.590000</td>\n",
       "      <td>6.060000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>40.200000</td>\n",
       "      <td>75.400000</td>\n",
       "      <td>33.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>82.200000</td>\n",
       "      <td>416.600000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>16.410000</td>\n",
       "      <td>9.670000</td>\n",
       "      <td>1079.100000</td>\n",
       "      <td>650.900000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>325.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age         ALB         ALP         AST         BIL         CHE  \\\n",
       "count  615.000000  614.000000  597.000000  615.000000  615.000000  615.000000   \n",
       "mean    47.408130   41.620195   68.283920   34.786341   11.396748    8.196634   \n",
       "std     10.055105    5.780629   26.028315   33.090690   19.673150    2.205657   \n",
       "min     19.000000   14.900000   11.300000   10.600000    0.800000    1.420000   \n",
       "25%     39.000000   38.800000   52.500000   21.600000    5.300000    6.935000   \n",
       "50%     47.000000   41.950000   66.200000   25.900000    7.300000    8.260000   \n",
       "75%     54.000000   45.200000   80.100000   32.900000   11.200000    9.590000   \n",
       "max     77.000000   82.200000  416.600000  324.000000  254.000000   16.410000   \n",
       "\n",
       "             CHOL         CREA         CGT        PROT         ALT  \n",
       "count  605.000000   615.000000  615.000000  614.000000  614.000000  \n",
       "mean     5.368099    81.287805   39.533171   72.044137   28.450814  \n",
       "std      1.132728    49.756166   54.661071    5.402636   25.469689  \n",
       "min      1.430000     8.000000    4.500000   44.800000    0.900000  \n",
       "25%      4.610000    67.000000   15.700000   69.300000   16.400000  \n",
       "50%      5.300000    77.000000   23.300000   72.200000   23.000000  \n",
       "75%      6.060000    88.000000   40.200000   75.400000   33.075000  \n",
       "max      9.670000  1079.100000  650.900000   90.000000  325.300000  "
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Analysis of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I will seek to understand and elaborate further on the HCV dataset and the data that is in it. By understanding missing values, statistical parameters, the types of characteristics, as well as the classification values, the aim is to better plan how to work efficiently with the dataset. As illustrated at it's web page at the UC Irvine's Machine Learning Repository, the data in the dataset is in the following format [1]:\n",
    "\n",
    "| Variable Name | Role     | Type       | Demographic | Description | Units | Missing Values |\n",
    "|---------------|----------|------------|-------------|-------------|-------|----------------|\n",
    "| ID            | ID       | Integer    |             |  Patient ID |       | no             |\n",
    "| Age           | Feature  | Integer    | Age         |             | years | no             |\n",
    "| Sex           | Feature  | Binary     | Sex         |             |       | no             |\n",
    "| ALB           | Feature  | Continuous |             |             |       | yes            |\n",
    "| ALP           | Feature  | Continuous |             |             |       | yes            |\n",
    "| AST           | Feature  | Continuous |             |             |       | yes            |\n",
    "| BIL           | Feature  | Continuous |             |             |       | no             |\n",
    "| CHE           | Feature  | Continuous |             |             |       | no             |\n",
    "| CHOL          | Feature  | Continuous |             |             |       | yes            |\n",
    "| CREA          | Feature  | Continuous |             |             |       | no             |\n",
    "| CGT           | Feature  | Continuous |             |             |       | no             |\n",
    "| PROT          | Feature  | Continuous |             |             |       | yes            |\n",
    "| Category      | Target   | Categorical|             | values: '0=Blood Donor', '0s=suspect Blood Donor', '1=Hepatitis', '2=Fibrosis', '3=Cirrhosis' |       | no             |\n",
    "| ALT           | Feature  | Continuous |             |             |       | no             |\n",
    "\n",
    "\n",
    "A quick note to be taken: For the features here the values are mostly continuous, which is somewhat expected as these are measurements gathered from the patients. The ID is a unique integer value, whilst the age is also of course a numerical-discrete value in integer form. The sex of the patients is a binary value of either 'm' or 'f'.\n",
    "This will all be further discussed later in section 2.\n",
    "\n",
    "#### 2.1.1 Missing Values Study\n",
    "As can be read in the table, the HCV dataset from UC Irvine's Machine Learning Repository contains several variables, some of which have missing values. In this section, I will focus on analyzing these missing values to understand their impact on the dataset and how they should be handled for effective machine learning model development. The variables with missing values are: ALB, ALP, AST, CHOL, and PROT. These are all continuous features, indicating that they are likely to represent some quantitative measurements. \n",
    "\n",
    "Understanding the extent of which values are missing within certain variables is crucial. It is important to calculate the proportion of missing values for each variable. If a significant proportion of data is missing in a particular variable, it might impact the reliability of any analysis involving that variable. So, let's dive deeper into this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No. values</th>\n",
       "      <th>No. null</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALB</th>\n",
       "      <td>614</td>\n",
       "      <td>1</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROT</th>\n",
       "      <td>614</td>\n",
       "      <td>1</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALT</th>\n",
       "      <td>614</td>\n",
       "      <td>1</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHOL</th>\n",
       "      <td>605</td>\n",
       "      <td>10</td>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALP</th>\n",
       "      <td>597</td>\n",
       "      <td>18</td>\n",
       "      <td>2.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      No. values  No. null     %\n",
       "ALB          614         1  0.16\n",
       "PROT         614         1  0.16\n",
       "ALT          614         1  0.16\n",
       "CHOL         605        10  1.63\n",
       "ALP          597        18  2.93"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALB, ALP, AST, CHOL, and PROT has missing values\n",
    "# Lets find the percentage of missing values for each of these variables\n",
    "def missing_percentage(df):\n",
    "    '''\n",
    "    This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The pandas object holding the data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    missing_values : Series\n",
    "        Total missing values of each feature.\n",
    "    '''\n",
    "    # Get the count of non null values of each feature\n",
    "    total = df.notnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n",
    "    total_null = df.isnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n",
    "    percent = np.round(df.isnull().sum().sort_values(ascending = False)/len(df)*100, 2)[np.round(df.isnull().sum().sort_values(ascending = False)/len(df)*100, 2) != 0]\n",
    "    return pd.concat([total, total_null, percent], axis = 1, keys = ['No. values', 'No. null', '%'])\n",
    "\n",
    "missing_percentage(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the result table above, it is apparent that the degree of which the values are missing is varying. I would devise my strategy based on this information. I also note that in the source it appears that ALT and AST have been swapped, and that it is in fact ALT that has a missing value.\n",
    "\n",
    "Firstly, since in the cases of the ALB, PROT, and ALT variables there are only one missing value for each, we could either perform a simple imputation strategy or just delete the missing instances. Whether deletion or imputation is the preferred choice, should depend on how skewed the dataset is. Let's understand this further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ALB        PROT         ALT        CHOL         ALP\n",
      "count  614.000000  614.000000  614.000000  605.000000  597.000000\n",
      "mean    41.620195   72.044137   28.450814    5.368099   68.283920\n",
      "std      5.780629    5.402636   25.469689    1.132728   26.028315\n",
      "min     14.900000   44.800000    0.900000    1.430000   11.300000\n",
      "25%     38.800000   69.300000   16.400000    4.610000   52.500000\n",
      "50%     41.950000   72.200000   23.000000    5.300000   66.200000\n",
      "75%     45.200000   75.400000   33.075000    6.060000   80.100000\n",
      "max     82.200000   90.000000  325.300000    9.670000  416.600000\n"
     ]
    }
   ],
   "source": [
    "# Find the range of each variable that has missing values\n",
    "def describe_missing_vals(vals = ['ALB', 'PROT', 'ALT', 'CHOL', 'ALP'], data = X):\n",
    "    print(data[vals].describe())\n",
    "\n",
    "describe_missing_vals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down these numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Albumin (ALB):\n",
    "- 1 missing value.\n",
    "- The mean and median are close, indicating a relatively symmetrical distribution.\n",
    "- Imputation is likely a good strategy here. Given the low percentage of missing data, mean or median imputation could work without significantly affecting the distribution.\n",
    "\n",
    "\n",
    "Protein (PROT):\n",
    "- 1 missing value.\n",
    "- Similar to ALB, the distribution seems symmetrical.\n",
    "- Again, mean or median imputation would be suitable due to the low percentage of missing data.\n",
    "\n",
    "\n",
    "Alanine Aminotransferase (ALT):\n",
    "- 1 missing value.\n",
    "- The standard deviation is quite large relative to the mean, indicating variability.\n",
    "- Given the data variability and that we have only a single missing entry, it might be a good solution to eliminate the entry with the missing data.\n",
    "\n",
    "\n",
    "Cholesterol (CHOL):\n",
    "- 10 missing values.\n",
    "- The mean and median are fairly close, but the standard deviation is relatively high - relative to the mean value. \n",
    "- Given the slightly higher percentage of missing data (1.63%), a simple imputation might still be reasonable. However, I will attempt to use a more sofisticated imputation for this value. \n",
    "\n",
    "\n",
    "Alkaline Phosphatase (ALP):\n",
    "- 18 missing values.\n",
    "- There's a significant difference between the mean and the median, indicating a skewed distribution.\n",
    "- The percentage of missing data is higher (2.93%). For skewed distributions, median imputation, or a more complex imputation method like regression imputation, could be considered to avoid introducing bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "             CHOL         ALP\n",
      "count  604.000000  596.000000\n",
      "mean     5.367053   68.304027\n",
      "std      1.133375   26.045538\n",
      "min      1.430000   11.300000\n",
      "25%      4.607500   52.500000\n",
      "50%      5.300000   66.250000\n",
      "75%      6.065000   80.125000\n",
      "max      9.670000  416.600000\n",
      "\n",
      " Imputed with Regression:\n",
      "             CHOL         ALP\n",
      "count  614.000000  614.000000\n",
      "mean     5.367053   68.304027\n",
      "std      1.124092   25.660291\n",
      "min      1.430000   11.300000\n",
      "25%      4.620000   52.925000\n",
      "50%      5.305000   66.700000\n",
      "75%      6.057500   79.300000\n",
      "max      9.670000  416.600000\n",
      "\n",
      "Imputed with KNN:\n",
      "             CHOL         ALP\n",
      "count  614.000000  614.000000\n",
      "mean     5.366708   68.242507\n",
      "std      1.124769   25.682554\n",
      "min      1.430000   11.300000\n",
      "25%      4.620000   52.900000\n",
      "50%      5.300000   66.300000\n",
      "75%      6.060000   79.300000\n",
      "max      9.670000  416.600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/hqn1sx256k77y1jwx4l6gnf00000gn/T/ipykernel_91484/1254858486.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['ALB'].fillna(X['ALB'].mean(), inplace = True)\n",
      "/var/folders/yf/hqn1sx256k77y1jwx4l6gnf00000gn/T/ipykernel_91484/1254858486.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['PROT'].fillna(X['PROT'].mean(), inplace = True)\n",
      "/var/folders/yf/hqn1sx256k77y1jwx4l6gnf00000gn/T/ipykernel_91484/1254858486.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.dropna(subset = ['ALT'], inplace = True)\n"
     ]
    }
   ],
   "source": [
    "# ALB and PROT\n",
    "# Imputing the missing value for ALB and PROT\n",
    "X['ALB'].fillna(X['ALB'].mean(), inplace = True)\n",
    "X['PROT'].fillna(X['PROT'].mean(), inplace = True)\n",
    "\n",
    "# ALT\n",
    "# Eliminating the entry with missing value for ALT\n",
    "X.dropna(subset = ['ALT'], inplace = True)\n",
    "\n",
    "# Create different versions of the dataset with different imputers\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# Imputing the missing values for CHOL and ALP using different imputers to create different versions of the dataset\n",
    "imputer_knn = KNNImputer(n_neighbors = 8)\n",
    "imputer_regression = SimpleImputer(strategy = 'mean')\n",
    "\n",
    "def get_imputed_data(imputer, df):\n",
    "    '''\n",
    "    This function takes an imputer object and a DataFrame as input and returns a DataFrame with the missing values imputed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imputer : Imputer\n",
    "        An imputer object.\n",
    "    df : DataFrame\n",
    "        The pandas object holding the data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        The pandas object holding the data with the missing values imputed.\n",
    "    '''\n",
    "    # Imputing CHOL and ALP\n",
    "    df_copy = df.copy()\n",
    "    df_copy[['CHOL', 'ALP']] = imputer.fit_transform(df_copy[['CHOL', 'ALP']])\n",
    "    return df_copy\n",
    "\n",
    "print(\"Original:\")\n",
    "describe_missing_vals(['CHOL', 'ALP'])\n",
    "\n",
    "print(\"\\n Imputed with Regression:\")\n",
    "describe_missing_vals(['CHOL', 'ALP'], get_imputed_data(imputer_regression, X))\n",
    "\n",
    "print(\"\\nImputed with KNN:\")\n",
    "describe_missing_vals(['CHOL', 'ALP'], get_imputed_data(imputer_knn, X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data from after imputing, the data seem to have kept a distribution that is close to the original. I will keep these versions and utilize them for other purposes later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature Modification of the Dataset Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As most of the values are continuous and not binary or numerical, there are not a lot of encoding needed for this dataset. However, the Sex feature can be label-encoded or one-hot encoded instead of being categorical with strings as identifiers.\n",
    "\n",
    "Now, there are two main concerns to consider: \n",
    "- label-encoding implies an ordinal relationship between the two values. However, even though the different sexes can each be assigned either a 0 or a 1 as a classification - there is no real mathematical relation or distance between the two. Thus, label-encoding provides a more simple transformation than that of a one-hot encoder, but may create biased results because of the assumption of a mathematical relation, even though 0='male' !< 1='female' and vice versa. \n",
    "- One-hot encoding introduces more dimensionality into the dataset, which might slow down some models. However, this should not be too much of a concern given that this dataset only introduces two classifications for sex. \n",
    "\n",
    "Given these reflections, since one-hot encoding does not introduce too great of a dimensionality increase, I will stick with one-hot encoding to avoid artificial ordinality and biased results based on a non-existent mathematical relation between the two sexes. \n",
    "\n",
    "*Below I am encoding the data using the ColumnTransformer in the sklearn library as suggested by Sunny Srinidhi [2].*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex_f</th>\n",
       "      <th>Sex_m</th>\n",
       "      <th>Age</th>\n",
       "      <th>ALB</th>\n",
       "      <th>ALP</th>\n",
       "      <th>AST</th>\n",
       "      <th>BIL</th>\n",
       "      <th>CHE</th>\n",
       "      <th>CHOL</th>\n",
       "      <th>CREA</th>\n",
       "      <th>CGT</th>\n",
       "      <th>PROT</th>\n",
       "      <th>ALT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>38.5</td>\n",
       "      <td>52.5</td>\n",
       "      <td>22.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6.93</td>\n",
       "      <td>3.23</td>\n",
       "      <td>106.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>69.0</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>38.5</td>\n",
       "      <td>70.3</td>\n",
       "      <td>24.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>11.17</td>\n",
       "      <td>4.80</td>\n",
       "      <td>74.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>76.5</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>46.9</td>\n",
       "      <td>74.7</td>\n",
       "      <td>52.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.84</td>\n",
       "      <td>5.20</td>\n",
       "      <td>86.0</td>\n",
       "      <td>33.2</td>\n",
       "      <td>79.3</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>43.2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>22.6</td>\n",
       "      <td>18.9</td>\n",
       "      <td>7.33</td>\n",
       "      <td>4.74</td>\n",
       "      <td>80.0</td>\n",
       "      <td>33.8</td>\n",
       "      <td>75.7</td>\n",
       "      <td>30.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>39.2</td>\n",
       "      <td>74.1</td>\n",
       "      <td>24.8</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.15</td>\n",
       "      <td>4.32</td>\n",
       "      <td>76.0</td>\n",
       "      <td>29.9</td>\n",
       "      <td>68.7</td>\n",
       "      <td>32.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>32.0</td>\n",
       "      <td>416.6</td>\n",
       "      <td>110.3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.57</td>\n",
       "      <td>6.30</td>\n",
       "      <td>55.7</td>\n",
       "      <td>650.9</td>\n",
       "      <td>68.5</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>24.0</td>\n",
       "      <td>102.8</td>\n",
       "      <td>44.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>3.02</td>\n",
       "      <td>63.0</td>\n",
       "      <td>35.9</td>\n",
       "      <td>71.3</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>29.0</td>\n",
       "      <td>87.3</td>\n",
       "      <td>99.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.63</td>\n",
       "      <td>66.7</td>\n",
       "      <td>64.2</td>\n",
       "      <td>82.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.56</td>\n",
       "      <td>4.20</td>\n",
       "      <td>52.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.07</td>\n",
       "      <td>5.30</td>\n",
       "      <td>67.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sex_f  Sex_m  Age   ALB    ALP    AST   BIL    CHE  CHOL   CREA    CGT  \\\n",
       "0        0      1   32  38.5   52.5   22.1   7.5   6.93  3.23  106.0   12.1   \n",
       "1        0      1   32  38.5   70.3   24.7   3.9  11.17  4.80   74.0   15.6   \n",
       "2        0      1   32  46.9   74.7   52.6   6.1   8.84  5.20   86.0   33.2   \n",
       "3        0      1   32  43.2   52.0   22.6  18.9   7.33  4.74   80.0   33.8   \n",
       "4        0      1   32  39.2   74.1   24.8   9.6   9.15  4.32   76.0   29.9   \n",
       "..     ...    ...  ...   ...    ...    ...   ...    ...   ...    ...    ...   \n",
       "609      1      0   62  32.0  416.6  110.3  50.0   5.57  6.30   55.7  650.9   \n",
       "610      1      0   64  24.0  102.8   44.4  20.0   1.54  3.02   63.0   35.9   \n",
       "611      1      0   64  29.0   87.3   99.0  48.0   1.66  3.63   66.7   64.2   \n",
       "612      1      0   46  33.0    NaN   62.0  20.0   3.56  4.20   52.0   50.0   \n",
       "613      1      0   59  36.0    NaN   80.0  12.0   9.07  5.30   67.0   34.0   \n",
       "\n",
       "     PROT    ALT  \n",
       "0    69.0    7.7  \n",
       "1    76.5   18.0  \n",
       "2    79.3   36.2  \n",
       "3    75.7   30.6  \n",
       "4    68.7   32.6  \n",
       "..    ...    ...  \n",
       "609  68.5    5.9  \n",
       "610  71.3    2.9  \n",
       "611  82.0    3.5  \n",
       "612  71.0   39.0  \n",
       "613  68.0  100.0  \n",
       "\n",
       "[614 rows x 13 columns]"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Assuming your dataframe is named 'df'\n",
    "\n",
    "# Create the ColumnTransformer\n",
    "transformer = ColumnTransformer(transformers=[('column_transformer', OneHotEncoder(), ['Sex'])], remainder='passthrough')\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_data = transformer.fit_transform(X)\n",
    "\n",
    "# Get the column names for the one-hot encoded columns\n",
    "encoded_feature_names = transformer.named_transformers_['column_transformer'].get_feature_names_out()\n",
    "\n",
    "# Combine the new and original column names, excluding 'Sex' as it is now one-hot encoded\n",
    "final_columns = list(encoded_feature_names) + list(X.drop('Sex', axis=1).columns)\n",
    "\n",
    "# Convert the numpy array back to a DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=final_columns)\n",
    "\n",
    "# Convert the one-hot encoded columns to integers and the Age column to int\n",
    "encoded_df[encoded_feature_names] = encoded_df[encoded_feature_names].astype(int)\n",
    "encoded_df['Age'] = encoded_df['Age'].astype(int)\n",
    "\n",
    "# Replace the original features with the encoded features\n",
    "X = encoded_df\n",
    "\n",
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have two versions of the dataset\n",
    "# 1. The dataset with missing values imputed using KNN\n",
    "# 2. The dataset with missing values imputed using regression\n",
    "dataset_versions = [get_imputed_data(imputer_knn, X), get_imputed_data(imputer_regression, X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important element to consider in using datasets for machine learning models, especially within the medical scene - is outliers in the dataset. Given that this dataset is based on results to identify HCV or more severe liver disease, concluding on whether outliers are disruptive or indicative is a little difficult.\n",
    "\n",
    "In this section I seek to gain an overview of the outlier scenery, without making any changes just yet. However, this knowledge can be important to understand undefined behaviour, false-postives, and false-negatives later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries within each target value:\n",
      "Category              \n",
      "0=Blood Donor             533\n",
      "3=Cirrhosis                30\n",
      "1=Hepatitis                24\n",
      "2=Fibrosis                 21\n",
      "0s=suspect Blood Donor      7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Number of outliers within each target value:\n",
      "Category              \n",
      "3=Cirrhosis               26\n",
      "0=Blood Donor             13\n",
      "1=Hepatitis               10\n",
      "0s=suspect Blood Donor     7\n",
      "2=Fibrosis                 6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Create the isolation forest model\n",
    "outlier_detector = IsolationForest(contamination=0.1, random_state=86)\n",
    "outlier_detector.fit(dataset_versions[0])\n",
    "\n",
    "# Predict the outliers\n",
    "outlier_predictions = outlier_detector.predict(dataset_versions[0])\n",
    "\n",
    "print(\"Number of entries within each target value:\")\n",
    "print(y.value_counts())\n",
    "print(\"\\n\")\n",
    "print(\"Number of outliers within each target value:\")\n",
    "print(y.iloc[np.where(outlier_predictions == -1)[0]].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3 Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further build on the analysis of this dataset and which preprocessing that may be plausible, I want to further analyse the variables in the dataset. These can, at this step, be described as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex_f</th>\n",
       "      <th>Sex_m</th>\n",
       "      <th>Age</th>\n",
       "      <th>ALB</th>\n",
       "      <th>ALP</th>\n",
       "      <th>AST</th>\n",
       "      <th>BIL</th>\n",
       "      <th>CHE</th>\n",
       "      <th>CHOL</th>\n",
       "      <th>CREA</th>\n",
       "      <th>CGT</th>\n",
       "      <th>PROT</th>\n",
       "      <th>ALT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.387622</td>\n",
       "      <td>0.612378</td>\n",
       "      <td>47.423453</td>\n",
       "      <td>41.614691</td>\n",
       "      <td>68.242507</td>\n",
       "      <td>34.789088</td>\n",
       "      <td>11.403909</td>\n",
       "      <td>8.194381</td>\n",
       "      <td>5.366708</td>\n",
       "      <td>81.293322</td>\n",
       "      <td>39.566775</td>\n",
       "      <td>72.058867</td>\n",
       "      <td>28.450814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.487605</td>\n",
       "      <td>0.487605</td>\n",
       "      <td>10.056115</td>\n",
       "      <td>5.779015</td>\n",
       "      <td>25.682554</td>\n",
       "      <td>33.117600</td>\n",
       "      <td>19.688388</td>\n",
       "      <td>2.206747</td>\n",
       "      <td>1.124769</td>\n",
       "      <td>49.796545</td>\n",
       "      <td>54.699280</td>\n",
       "      <td>5.390252</td>\n",
       "      <td>25.469689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.420000</td>\n",
       "      <td>1.430000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>44.800000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>38.800000</td>\n",
       "      <td>52.900000</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>6.932500</td>\n",
       "      <td>4.620000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>69.300000</td>\n",
       "      <td>16.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>41.900000</td>\n",
       "      <td>66.300000</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>8.260000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>76.850000</td>\n",
       "      <td>23.300000</td>\n",
       "      <td>72.200000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>45.200000</td>\n",
       "      <td>79.300000</td>\n",
       "      <td>32.900000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>9.592500</td>\n",
       "      <td>6.060000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>40.200000</td>\n",
       "      <td>75.400000</td>\n",
       "      <td>33.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>82.200000</td>\n",
       "      <td>416.600000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>16.410000</td>\n",
       "      <td>9.670000</td>\n",
       "      <td>1079.100000</td>\n",
       "      <td>650.900000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>325.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sex_f       Sex_m         Age         ALB         ALP         AST  \\\n",
       "count  614.000000  614.000000  614.000000  614.000000  614.000000  614.000000   \n",
       "mean     0.387622    0.612378   47.423453   41.614691   68.242507   34.789088   \n",
       "std      0.487605    0.487605   10.056115    5.779015   25.682554   33.117600   \n",
       "min      0.000000    0.000000   19.000000   14.900000   11.300000   10.600000   \n",
       "25%      0.000000    0.000000   39.000000   38.800000   52.900000   21.600000   \n",
       "50%      0.000000    1.000000   47.000000   41.900000   66.300000   25.900000   \n",
       "75%      1.000000    1.000000   54.000000   45.200000   79.300000   32.900000   \n",
       "max      1.000000    1.000000   77.000000   82.200000  416.600000  324.000000   \n",
       "\n",
       "              BIL         CHE        CHOL         CREA         CGT  \\\n",
       "count  614.000000  614.000000  614.000000   614.000000  614.000000   \n",
       "mean    11.403909    8.194381    5.366708    81.293322   39.566775   \n",
       "std     19.688388    2.206747    1.124769    49.796545   54.699280   \n",
       "min      0.800000    1.420000    1.430000     8.000000    4.500000   \n",
       "25%      5.300000    6.932500    4.620000    67.000000   15.700000   \n",
       "50%      7.300000    8.260000    5.300000    76.850000   23.300000   \n",
       "75%     11.200000    9.592500    6.060000    88.000000   40.200000   \n",
       "max    254.000000   16.410000    9.670000  1079.100000  650.900000   \n",
       "\n",
       "             PROT         ALT  \n",
       "count  614.000000  614.000000  \n",
       "mean    72.058867   28.450814  \n",
       "std      5.390252   25.469689  \n",
       "min     44.800000    0.900000  \n",
       "25%     69.300000   16.400000  \n",
       "50%     72.200000   23.000000  \n",
       "75%     75.400000   33.075000  \n",
       "max     90.000000  325.300000  "
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_versions[0].describe() # Only displaying the first version of the dataset for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basing purely on the description of the data - ALP, AST, BIL, CHE, CREA, CGT, and ALT are potential candidates for discretization due to their skewed distributions and the presence of outliers. Discretization of these variables might enhance the model's ability to identify patterns and robustness. Thus, I want to create two new versions for each of the previous versions -> discretized and non-discretized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretizing ALP, AST, BIL, CHE, CREA, CGT, and ALT\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.4 Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create more versions of the dataset with standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will work further on these to create different versions of the dataset\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "\n",
    "# Create different versions of the dataset with different scalers\n",
    "# Here I am using StandardScaler and RobustScaler because RobustScaler is more robust to outliers\n",
    "# and StandardScaler is more sensitive to outliers. This will help us study the effect of outliers on the dataset.\n",
    "scalers = [StandardScaler(), RobustScaler()]\n",
    "\n",
    "def get_scaled_data(scaler, df):\n",
    "    '''\n",
    "    This function takes a scaler object and a DataFrame as input and returns a DataFrame with the features scaled.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scaler : Scaler\n",
    "        A scaler object.\n",
    "    df : DataFrame\n",
    "        The pandas object holding the data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        The pandas object holding the data with the features scaled.\n",
    "    '''\n",
    "    # Scaling the features except the one-hot encoded features\n",
    "    df_copy = df.copy()\n",
    "    df_copy[df.drop(encoded_feature_names, axis=1).columns] = scaler.fit_transform(df.drop(encoded_feature_names, axis=1))\n",
    "    return df_copy\n",
    "\n",
    "# Create different versions of the dataset with different scalers\n",
    "dataset_versions_scaled = [get_scaled_data(scaler, df) for df in dataset_versions for scaler in scalers]\n",
    "\n",
    "# Print the differences between the original dataset and the different versions of the dataset\n",
    "def __print_scaled_versions():\n",
    "    for i in range(0, len(dataset_versions_scaled)):\n",
    "        print(\"#\"*20, \"\\nDataset version\", i+1, \":\\n\")\n",
    "        print(dataset_versions_scaled[i].describe(), \"\\n\")\n",
    "\n",
    "# Uncomment if you want to study the differences between the datasets\n",
    "#__print_scaled_versions() # Commented out for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our now *4 versions* of the dataset, we can move on to work further with the different datasets.\n",
    "\n",
    "These are the different datasets:\n",
    "1. The dataset with missing values imputed using KNN and scaled using StandardScaler\n",
    "4. The dataset with missing values imputed using KNN and scaled using RobustScaler\n",
    "5. The dataset with missing values imputed using regression and scaled using StandardScaler\n",
    "8. The dataset with missing values imputed using regression and scaled using RobustScaler\n",
    "\n",
    "These can now be accessed using `dataset_versions_scaled[i-1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature Count Reduction of the Dataset Versions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 11\n",
      "Explained variance ratio: [0.21460663 0.16079638 0.12031029 0.09615263 0.0840006  0.06764563\n",
      " 0.06268    0.05108518 0.04729501 0.03891188 0.02907983]\n",
      "Explained variance ratio sum: 0.9725640698098349\n"
     ]
    }
   ],
   "source": [
    "# Let's perform a principal component analysis on the dataset\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA object\n",
    "pca = PCA(n_components = 0.95)\n",
    "\n",
    "# Fit and transform the dataset\n",
    "pca_data = pca.fit_transform(dataset_versions_scaled[0])\n",
    "\n",
    "# Print the number of components\n",
    "print(\"Number of components:\", pca.n_components_)\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Explained variance ratio sum:\", sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Example Set Modification of the Dataset Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test sets\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3 Construction of Decision Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Instance Based Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Ensemble Learning with Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Balanced Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Yield Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5 Presentation and Defense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6 Bibliography\n",
    "\n",
    "[1] Lichtinghagen,Ralf, Klawonn,Frank, and Hoffmann,Georg. (2020). *HCV data*. UCI Machine Learning Repository. Accessed 13.01.24. https://doi.org/10.24432/C5D612.\n",
    "\n",
    "[2] Srinidhi, Sunny. (2019). *Use ColumnTransformer in SciKit instead of LabelEncoding and OneHotEncoding for data preprocessing in Machine Learning*. Towards Data Science. Accessed 31.01.24. https://towardsdatascience.com/columntransformer-in-scikit-for-labelencoding-and-onehotencoding-in-machine-learning-c6255952731b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
