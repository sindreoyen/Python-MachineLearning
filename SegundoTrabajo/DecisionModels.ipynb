{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Delivery 2: Learning Decision Models\n",
    "#### *by Sindre Øyen*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This paper will present a comprehensive analysis of the Hepatitis C Virus (HCV) dataset from the University of California, Irvine (UCI) Machine Learning Repository [1]. The focus of the paper will be to apply and evaluate various decision modeling techniques - encompassing preprocessing, model construction, and model evaluation. The paper will explore multiple preprocessing strategies, including handling missing values and feature modification, to prepare the dataset for the use in different machine learning models. These models will range from instance-based learning and decision trees to ensemble learning with trees and neural networks. The evaluation of the models will be based on a balanced construction, performance metrics, and yield curves, offering insights into their applicability in healthcare data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To initialize this study, the dataset itself can be loaded from the ICU database as such:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# Loading in the dataset\n",
    "hcv_data = fetch_ucirepo(id=571) \n",
    "  \n",
    "# Separating the features and target\n",
    "X = hcv_data.data.features \n",
    "y = hcv_data.data.targets \n",
    "  \n",
    "def printInfo():\n",
    "    print(hcv_data.metadata)  \n",
    "    print(hcv_data.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2 Preprocessing of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will perform a reasoned construction of various versions of the dataset. These will be possible to differentiate in the later performed work with the data to evaluate performance on different versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ALB</th>\n",
       "      <th>ALP</th>\n",
       "      <th>AST</th>\n",
       "      <th>BIL</th>\n",
       "      <th>CHE</th>\n",
       "      <th>CHOL</th>\n",
       "      <th>CREA</th>\n",
       "      <th>CGT</th>\n",
       "      <th>PROT</th>\n",
       "      <th>ALT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>38.5</td>\n",
       "      <td>52.5</td>\n",
       "      <td>22.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6.93</td>\n",
       "      <td>3.23</td>\n",
       "      <td>106.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>69.0</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>38.5</td>\n",
       "      <td>70.3</td>\n",
       "      <td>24.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>11.17</td>\n",
       "      <td>4.80</td>\n",
       "      <td>74.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>76.5</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>46.9</td>\n",
       "      <td>74.7</td>\n",
       "      <td>52.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.84</td>\n",
       "      <td>5.20</td>\n",
       "      <td>86.0</td>\n",
       "      <td>33.2</td>\n",
       "      <td>79.3</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>43.2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>22.6</td>\n",
       "      <td>18.9</td>\n",
       "      <td>7.33</td>\n",
       "      <td>4.74</td>\n",
       "      <td>80.0</td>\n",
       "      <td>33.8</td>\n",
       "      <td>75.7</td>\n",
       "      <td>30.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>m</td>\n",
       "      <td>39.2</td>\n",
       "      <td>74.1</td>\n",
       "      <td>24.8</td>\n",
       "      <td>9.6</td>\n",
       "      <td>9.15</td>\n",
       "      <td>4.32</td>\n",
       "      <td>76.0</td>\n",
       "      <td>29.9</td>\n",
       "      <td>68.7</td>\n",
       "      <td>32.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>62</td>\n",
       "      <td>f</td>\n",
       "      <td>32.0</td>\n",
       "      <td>416.6</td>\n",
       "      <td>110.3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.57</td>\n",
       "      <td>6.30</td>\n",
       "      <td>55.7</td>\n",
       "      <td>650.9</td>\n",
       "      <td>68.5</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>64</td>\n",
       "      <td>f</td>\n",
       "      <td>24.0</td>\n",
       "      <td>102.8</td>\n",
       "      <td>44.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>3.02</td>\n",
       "      <td>63.0</td>\n",
       "      <td>35.9</td>\n",
       "      <td>71.3</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>64</td>\n",
       "      <td>f</td>\n",
       "      <td>29.0</td>\n",
       "      <td>87.3</td>\n",
       "      <td>99.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.63</td>\n",
       "      <td>66.7</td>\n",
       "      <td>64.2</td>\n",
       "      <td>82.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>46</td>\n",
       "      <td>f</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.56</td>\n",
       "      <td>4.20</td>\n",
       "      <td>52.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>59</td>\n",
       "      <td>f</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.07</td>\n",
       "      <td>5.30</td>\n",
       "      <td>67.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>615 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age Sex   ALB    ALP    AST   BIL    CHE  CHOL   CREA    CGT  PROT    ALT\n",
       "0     32   m  38.5   52.5   22.1   7.5   6.93  3.23  106.0   12.1  69.0    7.7\n",
       "1     32   m  38.5   70.3   24.7   3.9  11.17  4.80   74.0   15.6  76.5   18.0\n",
       "2     32   m  46.9   74.7   52.6   6.1   8.84  5.20   86.0   33.2  79.3   36.2\n",
       "3     32   m  43.2   52.0   22.6  18.9   7.33  4.74   80.0   33.8  75.7   30.6\n",
       "4     32   m  39.2   74.1   24.8   9.6   9.15  4.32   76.0   29.9  68.7   32.6\n",
       "..   ...  ..   ...    ...    ...   ...    ...   ...    ...    ...   ...    ...\n",
       "610   62   f  32.0  416.6  110.3  50.0   5.57  6.30   55.7  650.9  68.5    5.9\n",
       "611   64   f  24.0  102.8   44.4  20.0   1.54  3.02   63.0   35.9  71.3    2.9\n",
       "612   64   f  29.0   87.3   99.0  48.0   1.66  3.63   66.7   64.2  82.0    3.5\n",
       "613   46   f  33.0    NaN   62.0  20.0   3.56  4.20   52.0   50.0  71.0   39.0\n",
       "614   59   f  36.0    NaN   80.0  12.0   9.07  5.30   67.0   34.0  68.0  100.0\n",
       "\n",
       "[615 rows x 12 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hcv_data.data.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Analysis of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I will seek to understand and elaborate further on the HCV dataset and the data that is in it. By understanding missing values, statistical parameters, the types of characteristics, as well as the classification values, the aim is to better plan how to work efficiently with the dataset. As illustrated at it's web page at the UC Irvine's Machine Learning Repository, the data in the dataset is in the following format [1]:\n",
    "\n",
    "| Variable Name | Role     | Type       | Demographic | Description | Units | Missing Values |\n",
    "|---------------|----------|------------|-------------|-------------|-------|----------------|\n",
    "| ID            | ID       | Integer    |             |  Patient ID |       | no             |\n",
    "| Age           | Feature  | Integer    | Age         |             | years | no             |\n",
    "| Sex           | Feature  | Binary     | Sex         |             |       | no             |\n",
    "| ALB           | Feature  | Continuous |             |             |       | yes            |\n",
    "| ALP           | Feature  | Continuous |             |             |       | yes            |\n",
    "| AST           | Feature  | Continuous |             |             |       | yes            |\n",
    "| BIL           | Feature  | Continuous |             |             |       | no             |\n",
    "| CHE           | Feature  | Continuous |             |             |       | no             |\n",
    "| CHOL          | Feature  | Continuous |             |             |       | yes            |\n",
    "| CREA          | Feature  | Continuous |             |             |       | no             |\n",
    "| CGT           | Feature  | Continuous |             |             |       | no             |\n",
    "| PROT          | Feature  | Continuous |             |             |       | yes            |\n",
    "| Category      | Target   | Categorical|             | values: '0=Blood Donor', '0s=suspect Blood Donor', '1=Hepatitis', '2=Fibrosis', '3=Cirrhosis' |       | no             |\n",
    "| ALT           | Feature  | Continuous |             |             |       | no             |\n",
    "\n",
    "\n",
    "#### 2.1.1 Missing Values Study\n",
    "As can be read in the table, the HCV dataset from UC Irvine's Machine Learning Repository contains several variables, some of which have missing values. In this section, I will focus on analyzing these missing values to understand their impact on the dataset and how they should be handled for effective machine learning model development. The variables with missing values are: ALB, ALP, AST, CHOL, and PROT. These are all continuous features, indicating that they are likely to represent some quantitative measurements. \n",
    "\n",
    "Understanding the extent of which values are missing within certain variables is crucial. It is important to calculate the proportion of missing values for each variable. If a significant proportion of data is missing in a particular variable, it might impact the reliability of any analysis involving that variable. So, let's dive deeper into this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No. values</th>\n",
       "      <th>No. null</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALB</th>\n",
       "      <td>614</td>\n",
       "      <td>1</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROT</th>\n",
       "      <td>614</td>\n",
       "      <td>1</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALT</th>\n",
       "      <td>614</td>\n",
       "      <td>1</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHOL</th>\n",
       "      <td>605</td>\n",
       "      <td>10</td>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALP</th>\n",
       "      <td>597</td>\n",
       "      <td>18</td>\n",
       "      <td>2.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      No. values  No. null     %\n",
       "ALB          614         1  0.16\n",
       "PROT         614         1  0.16\n",
       "ALT          614         1  0.16\n",
       "CHOL         605        10  1.63\n",
       "ALP          597        18  2.93"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALB, ALP, AST, CHOL, and PROT has missing values\n",
    "# Lets find the percentage of missing values for each of these variables\n",
    "def missing_percentage(df):\n",
    "    '''\n",
    "    This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The pandas object holding the data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    missing_values : Series\n",
    "        Total missing values of each feature.\n",
    "    '''\n",
    "    # Get the count of non null values of each feature\n",
    "    total = df.notnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n",
    "    total_null = df.isnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n",
    "    percent = np.round(df.isnull().sum().sort_values(ascending = False)/len(df)*100, 2)[np.round(df.isnull().sum().sort_values(ascending = False)/len(df)*100, 2) != 0]\n",
    "    return pd.concat([total, total_null, percent], axis = 1, keys = ['No. values', 'No. null', '%'])\n",
    "\n",
    "missing_percentage(hcv_data.data.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the result table above, it is apparent that the degree of which the values are missing is varying. I would devise my strategy based on this information. I also note that in the source it appears that ALT and AST have been swapped, and that it is in fact ALT that has a missing value.\n",
    "\n",
    "Firstly, since in the cases of the ALB, PROT, and ALT variables there are only one missing value for each, we could either perform a simple imputation strategy or just delete the missing instances. Whether deletion or imputation is the preferred choice, should depend on how skewed the dataset is. Let's understand this further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALB : min = 14.9 , max = 82.2\n",
      "Mode value for ALB is 39.0\n",
      "Median value for ALB is 41.95\n",
      "Mean value for ALB is 41.62019543973941\n",
      "The mean value is this percentage less than the max value: 49.3671588324338\n",
      "The mean value is this percentage more than the min value: 179.33017073650612 \n",
      "\n",
      "PROT : min = 44.8 , max = 90.0\n",
      "Mode value for PROT is 71.9\n",
      "Median value for PROT is 72.2\n",
      "Mean value for PROT is 72.0441368078176\n",
      "The mean value is this percentage less than the max value: 19.950959102424896\n",
      "The mean value is this percentage more than the min value: 60.81280537459285 \n",
      "\n",
      "ALT : min = 0.9 , max = 325.3\n",
      "Mode value for ALT is 16.6\n",
      "Median value for ALT is 23.0\n",
      "Mean value for ALT is 28.450814332247557\n",
      "The mean value is this percentage less than the max value: 91.25397653481477\n",
      "The mean value is this percentage more than the min value: 3061.201592471951 \n",
      "\n",
      "CHOL : min = 1.43 , max = 9.67\n",
      "Mode value for CHOL is 5.07\n",
      "Median value for CHOL is 5.3\n",
      "Mean value for CHOL is 5.368099173553719\n",
      "The mean value is this percentage less than the max value: 44.48708196945482\n",
      "The mean value is this percentage more than the min value: 275.3915505981622 \n",
      "\n",
      "ALP : min = 11.3 , max = 416.6\n",
      "Mode value for ALP is 52.5\n",
      "Median value for ALP is 66.2\n",
      "Mean value for ALP is 68.28391959798995\n",
      "The mean value is this percentage less than the max value: 83.60923677436631\n",
      "The mean value is this percentage more than the min value: 504.28247431849513 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the range of each variable that has missing values\n",
    "for i in ['ALB', 'PROT', 'ALT', 'CHOL', 'ALP']:\n",
    "    print(i, ':', 'min =', hcv_data.data.features[i].min(), ', max =', hcv_data.data.features[i].max())\n",
    "    print(\"Mode value for\", i, \"is\", hcv_data.data.features[i].mode()[0])\n",
    "    print(\"Median value for\", i, \"is\", hcv_data.data.features[i].median())\n",
    "    print(\"Mean value for\", i, \"is\", hcv_data.data.features[i].mean())\n",
    "    print(\"The mean value is this percentage less than the max value:\", (hcv_data.data.features[i].max() - hcv_data.data.features[i].mean())/hcv_data.data.features[i].max()*100)\n",
    "    print(\"The mean value is this percentage more than the min value:\", (hcv_data.data.features[i].mean() - hcv_data.data.features[i].min())/hcv_data.data.features[i].min()*100, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down these numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*One missing value*\n",
    "- One data entry has a missing ALB value. The mode, median, and mean values are quite close percentage-wise. However, the skewedness is quite prominent with the largest value being 180% larger than the mean. It is possible that replacing the value with the mean will cause incorrect treatment of the data, thus it can be better to eliminate the entry with the missing data.\n",
    "- With regard to PROT, the mode, median, and the mean value are quite close and the skewedness is not too large, here it can be okay to perform imputation with the mean value.\n",
    "- With regard to ALT, the mode, median, and mean value are quite skewed, and the general skewedness is extreme - making it better to just eliminate this entry.\n",
    "\n",
    "*Several missing values*\n",
    "\n",
    "With regard to CHOL and ALP, which both have a greater amount of missing values - it can be important to try to understand why. I will attempt to use a more sofisticated imputation approach for these values, such as e.g., regression imputation to understand whether the missingness is correlated to the other variables in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/hqn1sx256k77y1jwx4l6gnf00000gn/T/ipykernel_26835/4063617870.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hcv_data.data.features.dropna(subset = ['ALB', 'ALT'], inplace = True)\n",
      "/var/folders/yf/hqn1sx256k77y1jwx4l6gnf00000gn/T/ipykernel_26835/4063617870.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hcv_data.data.features['PROT'].fillna(hcv_data.data.features['PROT'].mean(), inplace = True)\n"
     ]
    }
   ],
   "source": [
    "# Eliminating the entries with missing values for ALB and ALT\n",
    "hcv_data.data.features.dropna(subset = ['ALB', 'ALT'], inplace = True)\n",
    "# Imputing the missing value for PROT\n",
    "hcv_data.data.features['PROT'].fillna(hcv_data.data.features['PROT'].mean(), inplace = True)\n",
    "\n",
    "# Create different versions of the dataset with different imputers\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# Imputing the missing values for CHOL and ALP using different imputers to create different versions of the dataset\n",
    "imputer_knn = KNNImputer(n_neighbors = 5)\n",
    "imputer_regression = SimpleImputer(strategy = 'mean')\n",
    "\n",
    "def get_imputed_data(imputer, df):\n",
    "    '''\n",
    "    This function takes an imputer object and a DataFrame as input and returns a DataFrame with the missing values imputed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imputer : Imputer\n",
    "        An imputer object.\n",
    "    df : DataFrame\n",
    "        The pandas object holding the data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        The pandas object holding the data with the missing values imputed.\n",
    "    '''\n",
    "    # Imputing CHOL and ALP\n",
    "    df['CHOL'] = imputer.fit_transform(df[['CHOL']])\n",
    "    df['ALP'] = imputer.fit_transform(df[['ALP']])\n",
    "    return df\n",
    "\n",
    "missing_percentage(get_imputed_data(imputer_knn, hcv_data.data.features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.1.3 Type of Characteristics in the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Study of the Classification Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Missing Value Processing of the Dataset Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Feature Modification of the Dataset Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Feature Count Reduction of the Dataset Versions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Example Set Modification of the Dataset Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3 Construction of Decision Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Instance Based Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Ensemble Learning with Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Balanced Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Yield Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5 Presentation and Defense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6 Bibliography\n",
    "\n",
    "[1] Lichtinghagen,Ralf, Klawonn,Frank, and Hoffmann,Georg. (2020). HCV data. UCI Machine Learning Repository. https://doi.org/10.24432/C5D612."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
