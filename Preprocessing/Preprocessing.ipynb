{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Introduction\n",
    "In this paper, I will explore the realm of data discretization, an important preprocessing step in machine learning and data analysis. Firstly, I will elaborate on the discretization techniques included in the scikit-learn library. Secondly, I will implement an alternative approach to discretization based on decision trees, as described in the paper [1].\n",
    "\n",
    "[1] Niculescu-Mizil, A., Perlich, C., Swirszcz, G., Sindhwani, V., Liu, Y., Melville, P., Wang, D., Xiao, J., Hu, J., Singh, M., Xiong Shang, W., Feng Zhu, Y.. Winning the KDD Cup Orange Challenge with Ensemble Selection in Proceedings of KDD-Cup 2009 Competition, PMLR 7:23-34, 2009.\n",
    "https://dl--acm--org.us.debiblio.com/doi/10.5555/3000364.3000366"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Discretization in the scikit-learn Library\n",
    "*Investigate the use of the different discretization strategies included in scikit-learn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn library comprises some different discretization algorithms i.e., *K-bins discretization* and *Feature binarization* (https://scikit-learn.org/stable/modules/preprocessing.html#discretization, accessed on 11.12.23). I will explain further how they function and what their different use cases are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 K-Bins Discretization\n",
    "K-bins discretization is a preprocessing-technique based on turning data with continuous variables into categorical data. This is done by dividing the data into K intervals, namely bins. The bins can be of equal width or based on a custom criterion, decided by the strategy param of the `sklearn.preprocessing.KBinsDiscretizer`. The alternative states of this param is either *uniform*, *quantile*, or *kmeans*. The *uniform* strategy uses bins of equal and constant width; the *quantile* strategy uses the quantiles to create bins with the same population for each feature; the *kmeans* strategy creates the bins based on a strategy where all the values in the same bin have the same nearest center of a one-dimensional (1D) k-means cluster. Now, the latter strategy may sound quite greek at first. However, simply put, k-means clustering is just a method for grouping data points based on their proximity to eachother.  \n",
    "\n",
    "- K-bins discretization is especially useful for linear models. This is because it improves the linear models work with continuous data by converting the continuous data into categorical data. In the example shown below (https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization.html#sphx-glr-auto-examples-preprocessing-plot-discretization-py, accessed on 12.12.23), you can see the before and after effects of k-bins discretization on a decision tree, as well as a linear model. As is apparent, the linear model becomes much more flexible, and the decision tree less flexible.\n",
    "- To reduce the risk of overfitting, as you can visibly imagine based on the illustration below - it can be important to ensure a sufficient width of the different bins as a means to reduce overfitting. \n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_discretization_001.png\" width=\"600\" />\n",
    "\n",
    "#### 1.2 Feature Binarization\n",
    "Feature binarization is a preprocessing-technique that creates a threshold value for continuous data and separates the data into a binary distribution, based on which side of the threshold any given data point is located. \n",
    "- The use cases for feature binarization can be many. One trivial use case to think about is e.g., in medical situations, where it is interesting to analyse if some values are over a given threshold rather than the exact amount of a value. Binarizing continuous data can also be efficient in many other cases where it is useful to create a binary distribution based on a threshold. \n",
    "- Utilizing feature binarization can also improve computational efficacy, as it can lead to smaller and fewer numbers to work with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 Decision-Tree Discretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Fitting the Model\n",
    "*In this section I will be defining the function `decisionTreeDiscretizerFit(X_data,y_data,variables)`. This model, given a dataset and its classification values (`X_data` and `y_data`), and a list of feature indices, `variables`, returns a dictionary that associates each feature index in `variables` with a pair `(treeModel,encoding)`. Here, `treeModel` is a decision tree trained only with that feature of the dataset `X_data` with classification values `y_data`, and `encoding` is an association between the different classification probability vectors (*predict_proba*) obtained with that tree in the dataset and unique numerical values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTreeDiscretizerFit(X_data, y_data, variables: list) -> dict:\n",
    "    '''\n",
    "    This method fits a decision tree discretizer for each of the features in `X_data` which are in the `variables` list.\n",
    "\n",
    "    param X_data: array-like \n",
    "        The input features dataset.\n",
    "    param y_data: array-like\n",
    "        The target values (class labels).\n",
    "    param variables: list\n",
    "        List of indices of features to be discretized.\n",
    "\n",
    "    return: dict\n",
    "        A dictionary where keys are feature indices and values are tuples (treeModel, encoding).\n",
    "    '''\n",
    "    tree_discretizers = {}\n",
    "\n",
    "    # If no variables are specified, we take into account all features\n",
    "    if variables is None:\n",
    "        print(\"No variables specified. All features will be discretized.\")\n",
    "        variables = list(range(X_data.shape[1]))\n",
    "    \n",
    "    for v in variables:\n",
    "        # Initializing the best tree and its score \n",
    "        best_tree, best_score = None, -np.inf\n",
    "\n",
    "        # Isolating the feature column for training\n",
    "        feature_column = np.array(X_data[:, v]).reshape(-1, 1)\n",
    "\n",
    "        # Training a decision tree for an array of possible depths to find the best one\n",
    "        for depth in range(1, 10):\n",
    "            # Training the decision tree with the current depth\n",
    "            tree = DecisionTreeClassifier(max_depth=depth)\n",
    "            tree.fit(feature_column, y_data)\n",
    "            # Calculating the score of the decision tree\n",
    "            score = tree.score(feature_column, y_data)\n",
    "            # Updating the best tree if the current one is better\n",
    "            if score > best_score:\n",
    "                best_tree = tree\n",
    "                best_score = score            \n",
    "\n",
    "        # Generating the different possible classification probability vectors\n",
    "        prob_vectors = best_tree.predict_proba(feature_column)\n",
    "        unique_prob_vectors = np.unique(prob_vectors, axis=0)\n",
    "        \n",
    "        # Associating each unique probability vector with its index\n",
    "        encoding = { tuple(vector): index for index, vector in enumerate(unique_prob_vectors) }\n",
    "\n",
    "        # Adding the best tree (treeModel) and its encoding to the result dictionary i.e., { feature_index: (treeModel, encoding) }\n",
    "        tree_discretizers[v] = (best_tree, encoding)\n",
    "\n",
    "    return tree_discretizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Creating a Transform Method\n",
    "*In this section I will be defining the function `decisionTreeDiscretizerTransform(X_data,variables,dtDiscretizer)`. This model, given a dataset, `X_data`, a list of feature indices, `variables`, and a dictionary `dtDiscretizer` obtained by the `decisionTreeDiscretizerFit` function (not necessarily for the same dataset `X_data` or the same list of variables `variables`), generates a new dataset (i.e., does not modify the input dataset) identical to `X_data`. In this new dataset, the values of the features whose indices are indicated in `variables` are replaced by the numerical values associated with the classification probability vectors, obtained with the corresponding decision trees associated with each feature in `dtDiscretizer`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTreeDiscretizerTransform(X_data, variables: list, tree_discretizers: dict) -> np.ndarray:\n",
    "    '''\n",
    "    This method transforms the features in `X_data` using the decision tree discretizers in `tree_discretizers`.\n",
    "\n",
    "    param X_data: array-like \n",
    "        The input features dataset.\n",
    "    param variables: list\n",
    "        List of indices of features to be discretized.\n",
    "    param dtDiscretizer: dict\n",
    "        A dictionary where keys are feature indices and values are tuples (treeModel, encoding).\n",
    "\n",
    "    return: np.ndarray\n",
    "        The transformed dataset.\n",
    "    '''\n",
    "    # Copying the input dataset\n",
    "    X_data_discretized = X_data.copy()\n",
    "\n",
    "    # If no variables are specified, we discretize all features\n",
    "    if variables is None:\n",
    "        variables = list(range(X_data.shape[1]))\n",
    "\n",
    "    # Iterating over the features to be discretized\n",
    "    for v in variables:\n",
    "        # Isolating the data for this feature by selecting its column\n",
    "        feature_column = np.array(X_data_discretized[:, v]).reshape(-1, 1)\n",
    "        # Transforming the feature column using the correct decision tree discretizer\n",
    "        transformed_column = np.array([tree_discretizers[v][1][tuple(vector)] for vector in tree_discretizers[v][0].predict_proba(feature_column)])\n",
    "        # Replacing the feature column with the transformed one\n",
    "        X_data_discretized[:, v] = transformed_column\n",
    "\n",
    "    return X_data_discretized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Testing the discretizer ##########\n",
      "Feature 'mean radius': Discretized from 456 to 10 values.\n",
      "Feature 'mean texture': Discretized from 479 to 13 values.\n",
      "Feature 'mean perimeter': Discretized from 522 to 10 values.\n",
      "Feature 'mean area': Discretized from 539 to 11 values.\n",
      "Feature 'mean smoothness': Discretized from 474 to 13 values.\n",
      "Feature 'mean compactness': Discretized from 537 to 11 values.\n",
      "Feature 'mean concavity': Discretized from 537 to 12 values.\n",
      "Feature 'mean concave points': Discretized from 542 to 9 values.\n",
      "Feature 'mean symmetry': Discretized from 432 to 20 values.\n",
      "\n",
      " --> Test passed! All other features are not discretized.\n"
     ]
    }
   ],
   "source": [
    "# Testing the discretizer\n",
    "if __name__ == '__main__':\n",
    "    # Loading the dataset\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    dataset = load_breast_cancer()\n",
    "    X_data = dataset.data\n",
    "    y_data = dataset.target\n",
    "\n",
    "    variables = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "    # Fitting the discretizer\n",
    "    tree_discretizers = decisionTreeDiscretizerFit(X_data, y_data, variables)\n",
    "    X_data_discretized = decisionTreeDiscretizerTransform(X_data, variables, tree_discretizers)\n",
    "\n",
    "    print(\"#\"*10 + \" Testing the discretizer \" + \"#\"*10)\n",
    "    for v in variables: \n",
    "        print(f\"Feature '{dataset.feature_names[v]}': Discretized from {len(np.unique(X_data[:, v]))} to {len(np.unique(X_data_discretized[:, v]))} values.\")\n",
    "    \n",
    "    # Testing that all other values are not discretized\n",
    "    for v in range(X_data.shape[1]):\n",
    "        if v not in variables:\n",
    "            # This throws an assertion error if the two arrays are not equal\n",
    "            assert np.array_equal(X_data[:, v], X_data_discretized[:, v])\n",
    "    print(\"\\n --> Test passed! All other features are not discretized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 Analysis and Comparison of Results\n",
    "To conclude the work in this short paper, I will compare the different discretization strategies included in *scikit-learn* and the decision-tree based implementation. For the comparison I will be using the *Iris* dataset and discretize some of its features. I will split the dataset into `X_train` and `X_test`, train the discretizers with `X_train` and transform both `X_train` and `X_test`. Finally, I will evaluate the performance of a *LogisticRegression* linear model, both with and without discretization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in the iris dataset (there are 4):\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loading the iris dataset\n",
    "iris = load_iris()\n",
    "def get_iris():\n",
    "    '''\n",
    "    Provides the data and target of the iris dataset, shuffled randomly.\n",
    "    \n",
    "    return: tuple\n",
    "        A tuple (X_train, X_test, y_train, y_test) where X_train and X_test are the training and test features, respectively, and y_train and y_test are the training and test targets, respectively.\n",
    "    '''\n",
    "    return train_test_split(iris.data, iris.target, test_size=0.33)\n",
    "\n",
    "print(f\"Features in the iris dataset (there are {len(iris.feature_names)}):\")\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Base Function for Analysing the Performance of Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a logistic regression model on the data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def trainLogisticRegression(X_train, y_train, X_test, y_test, prefix: str = \"\"):\n",
    "    # Training the model\n",
    "    model = LogisticRegression(max_iter=1500)\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predicting and calculating accuracies\n",
    "    y_pred_test, y_pred_train = model.predict(X_test), model.predict(X_train)\n",
    "    test_accuracy, train_accuracy = accuracy_score(y_test, y_pred_test), accuracy_score(y_train, y_pred_train)\n",
    "    print(\"#\"*10 + f\" Getting the results for the model, '{prefix}' \" + \"#\"*10)\n",
    "    print(f\"Accuracy for the model on the test-set: {test_accuracy*100:.2f}%\")\n",
    "    print(f\"Accuracy for the model on the train-set: {train_accuracy*100:.2f}%\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variables to be discretized for the iris dataset\n",
    "variables = [1,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Results of Decision-Tree Discretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finding the results from discretizing the data and training a LinearRegression model with the discretized data vs. not discretizing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Getting the results for the model, 'Discretized with decision tree' ##########\n",
      "Accuracy for the model on the test-set: 100.00%\n",
      "Accuracy for the model on the train-set: 97.00%\n",
      "\n",
      "########## Getting the results for the model, 'Original data' ##########\n",
      "Accuracy for the model on the test-set: 100.00%\n",
      "Accuracy for the model on the train-set: 94.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = get_iris()\n",
    "\n",
    "# Discretizing with the decision tree discretized\n",
    "tree_discretizers = decisionTreeDiscretizerFit(X_train, y_train, variables)\n",
    "X_train_discretized = decisionTreeDiscretizerTransform(X_train, variables=variables, tree_discretizers=tree_discretizers)\n",
    "X_test_discretized = decisionTreeDiscretizerTransform(X_test, variables=variables, tree_discretizers=tree_discretizers)\n",
    "\n",
    "# Training a logistic regression model on the discretized data\n",
    "trainLogisticRegression(X_train_discretized, y_train, X_test_discretized, y_test, prefix=\"Discretized with decision tree\")\n",
    "\n",
    "# Training a logistic regression model on the original data\n",
    "trainLogisticRegression(X_train, y_train, X_test, y_test, prefix=\"Original data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Having tested with some different values and shuffled datasets, I can conclude that in some cases the model with the discretized data set is better than the one without. However, for most cases the model with the undiscretized data seems to deliver the most consistent results with the least deviation. This is at least my observations from testing with the iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Results with K-Bins Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Getting the results for the model, 'Discretized with k-bins' ##########\n",
      "Accuracy for the model on the test-set: 96.00%\n",
      "Accuracy for the model on the train-set: 94.00%\n",
      "\n",
      "########## Getting the results for the model, 'Original data' ##########\n",
      "Accuracy for the model on the test-set: 94.00%\n",
      "Accuracy for the model on the train-set: 97.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implement k-bins discretization\n",
    "X_train, X_test, y_train, y_test = get_iris()\n",
    "\n",
    "# Discretizing with the decision tree discretized\n",
    "discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='kmeans', subsample=None)\n",
    "discretizer.fit(X_train[:, variables])\n",
    "\n",
    "# Transforming the data\n",
    "X_train_discretized = discretizer.transform(X_train[:, variables])\n",
    "X_test_discretized = discretizer.transform(X_test[:, variables])\n",
    "\n",
    "# Training a logistic regression model on the discretized data\n",
    "trainLogisticRegression(X_train_discretized, y_train, X_test_discretized, y_test, prefix=\"Discretized with k-bins\")\n",
    "\n",
    "# Training a logistic regression model on the original data\n",
    "trainLogisticRegression(X_train, y_train, X_test, y_test, prefix=\"Original data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ My impression is that the results here are quite similar to that of the decision-tree discretizer. There are no obvious pros / cons for either strategy (discretized vs not discretized): they both deviate and they take turns in out-performing each other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
