{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delivery: Linear Models (Part 1) - by Sindre Øyen\n",
    "\n",
    "In this part 1 of the delivery, I will explore methods of utilizing machine learning to understand \"feelings\" in texts, to understand whether the text is negatively loaded, or positively. The models used will be trained with the dataset aclImdb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In exercise 1, the data is loaded from the training and test sets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initial setups and helper methods*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printInfo(reviews_data, text_data, y_data):\n",
    "    '''\n",
    "    Prints information about the input data.\n",
    "\n",
    "    Parameters:\n",
    "    reviews_data (sklearn.utils.Bunch): A dictionary-like object that contains the reviews dataset.\n",
    "    text_data (numpy.ndarray): An array of strings containing the text data.\n",
    "    y_data (numpy.ndarray): An array of integers containing the labels for the text data.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    print(\"Tipo de text_data: {}\".format(type(text_data)))\n",
    "    print(\"Tipo de y_data: {}\".format(type(y_data)))\n",
    "    print(\"Cantidad de textos en el conjunto de entrenamiento: {}\".format(len(text_data)))\n",
    "    print(\"Etiquetas: {}\".format(reviews_data.target_names))\n",
    "\n",
    "    print(\"text_data[6]:\\n{}\\n\".format(text_data[6]))\n",
    "    print(\"y_data[6]: {}\\n\".format(y_data[6]))\n",
    "    print(\"Etiqueta asociada: {}\".format(reviews_data.target_names[y_data[6]]))\n",
    "\n",
    "    print(\"text_data[25]:\\n{}\\n\".format(text_data[25]))\n",
    "    print(\"y_data[25]: {}\\n\".format(y_data[25]))\n",
    "    print(\"Etiqueta asociada: {}\".format(reviews_data.target_names[y_data[25]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Creating a general method for loading folders from the IMDB dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "def loadSet(datafolderName: str, shouldPrint: bool = False) -> tuple:\n",
    "    '''\n",
    "    Loads the data from the specified folder of the IMDB set.\n",
    "\n",
    "    Parameters:\n",
    "    datafolderName (str): The name of the folder that contains the data.\n",
    "    shouldPrint (bool): A boolean value that indicates if the information about the data should be printed.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the text data and the labels.\n",
    "    '''\n",
    "    datafolder = os.path.join(\"aclImdb\", datafolderName)\n",
    "    reviews_data = load_files(datafolder)\n",
    "    text_data, y_data = reviews_data.data, reviews_data.target\n",
    "    text_data = [doc.replace(b\"<br />\", b\" \") for doc in text_data]\n",
    "    if shouldPrint:\n",
    "        printInfo(reviews_data, text_data, y_data)\n",
    "    return text_data, y_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Loading the training and test data, and counting the instances in each one*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de instancias en el conjunto de entrenamiento: 48840\n",
      "Numero de instancias en el conjunto de prueba: 25000\n"
     ]
    }
   ],
   "source": [
    "# Loading the training set\n",
    "text_train, y_train = loadSet(\"train\")\n",
    "# Loading the test set\n",
    "text_test, y_test = loadSet(\"test\")\n",
    "\n",
    "print(f\"Numero de instancias en el conjunto de entrenamiento: {len(text_train)}\")\n",
    "print(f\"Numero de instancias en el conjunto de prueba: {len(text_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 2\n",
    "*Utilizing ***CountVectorizer*** on an example data set of four sentences*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***CountVectorizer*** takes in datasets in the form of an array of different sentences. It uses these sentences to create a mapping over the unique words in the data-sets and then returns an vector of dimensions *NxW* with *N* being the length of the initial array, and *W* being the count of unique words. For each sentence it maps out how many occurences each unique word has in the given sentence, thus creating a vectorized representation of the correlations of the different sentences, that can be used in ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can se by running the code below, this CountVectorizer instance with the example data returns a *4x18* matrix consisting of 4 vectors of length 18, mapping the counts for each word in it's vocabulary, to the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer vocabulary: ['al' 'camión' 'cargamento' 'color' 'dañado' 'de' 'el' 'en' 'entrega'\n",
      " 'fuego' 'gritó' 'la' 'llegó' 'oro' 'plata' 'por' 'un' 'ver']\n",
      "\n",
      "CountVectorizer matrix:\n",
      "[[0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 1 1 1 1 0 0 2 1 0 2 0 0 0]\n",
      " [0 1 1 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0]\n",
      " [1 1 0 0 0 0 1 0 0 0 1 0 0 3 0 0 0 1]]\n",
      "\n",
      "CountVectorizer dimensions: (4, 18)\n"
     ]
    }
   ],
   "source": [
    "cuatro_frases =[\"Cargamento de oro dañado por el fuego\",\n",
    "              \"La entrega de la plata llegó en el camión color plata\",\n",
    "              \"El cargamento de oro llegó en un camión\",\n",
    "              \"Oro, oro, oro: gritó al ver el camión\"]\n",
    "\n",
    "example_vectorizer = CountVectorizer()\n",
    "example_vectorizer.fit(cuatro_frases)\n",
    "example_vector = example_vectorizer.transform(cuatro_frases)\n",
    "# Print data to analyze the results\n",
    "print(f\"CountVectorizer vocabulary: {example_vectorizer.get_feature_names_out()}\")\n",
    "print(f\"\\nCountVectorizer matrix:\\n{example_vector.toarray()}\\n\")\n",
    "print(f\"CountVectorizer dimensions: {example_vector.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 3\n",
    "Using CountVectorizer to vectorize the IMDB training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better generalize the data set I am going to utilize the *min_df* and *stop_words* paramts for the CountVectorizer initalizer. First I am going to explain how each of these parameters works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### min_df\n",
    "*min_df* is a float parameter that defines a rule for when a word should be included in the vectorizer's vocabulary, based on how many of the training documents (or sentences in this case) the word must be in to be included. The floating value is a value between 0 and 1, meaning 0 < *min_df* ≤ 1, where *min_df*=0.01 for example would mean that a word has to be in at least 1% of all sentences to be included in the vocabulary. Editing this parameter can help with reducing overfitting as a result of too specific data that does not occur that much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stop_words\n",
    "*stop_words* is a parameter that represents a set of words that will be ignored in the vocabulary. The reason for why one would use this parameter is that the count of some words (e.g., filler words such as \"in\", \"at\", \"this\"), does not necessarily represent a \"meaning\" in the data and treating them as such might lead to unwanted results. The scikit-learn library has a built in ENGLISH_STOP_WORDS set of words to use for the *stop_words* parameter. Using the stop_words parameters leads to fewer features in the vocabulary, leading to better opportunities for a generalized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (48840, 1068)\n",
      "X_test.shape: (25000, 1068)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=0.015, stop_words='english')\n",
    "vectorizer.fit(text_train)\n",
    "X_train = vectorizer.transform(text_train)\n",
    "X_test = vectorizer.transform(text_test)\n",
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"X_test.shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Training classifiers on the vectorized data. Specifically training on the classifiers ***LogisticRegression*** and ***LinearSVC***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In training these models, I will be utilizing the *C* regularization parameter. This parameter controls the inverse strength of the regularization, meaning that a lower *C* value will lead to stronger regularization and a higher *C* value will lead to less regularization. This effectively means that:\n",
    "- A low C value can lead to low bias but high variance (or overfitting)\n",
    "- A high C value can lead to high bias but low variance (or underfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-27 {color: black;}#sk-container-id-27 pre{padding: 0;}#sk-container-id-27 div.sk-toggleable {background-color: white;}#sk-container-id-27 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-27 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-27 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-27 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-27 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-27 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-27 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-27 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-27 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-27 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-27 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-27 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-27 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-27 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-27 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-27 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-27 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-27 div.sk-item {position: relative;z-index: 1;}#sk-container-id-27 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-27 div.sk-item::before, #sk-container-id-27 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-27 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-27 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-27 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-27 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-27 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-27 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-27 div.sk-label-container {text-align: center;}#sk-container-id-27 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-27 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-27\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.65, max_iter=10000, random_state=12)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" checked><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.65, max_iter=10000, random_state=12)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.65, max_iter=10000, random_state=12)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=0.65, random_state=12, max_iter=10000)\n",
    "logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sindreoyen/Desktop/Sevilla/Aprendizaje Automatico/Excercises/Deliveries/.venv/lib/python3.10/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/Users/sindreoyen/Desktop/Sevilla/Aprendizaje Automatico/Excercises/Deliveries/.venv/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-28 {color: black;}#sk-container-id-28 pre{padding: 0;}#sk-container-id-28 div.sk-toggleable {background-color: white;}#sk-container-id-28 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-28 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-28 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-28 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-28 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-28 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-28 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-28 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-28 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-28 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-28 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-28 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-28 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-28 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-28 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-28 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-28 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-28 div.sk-item {position: relative;z-index: 1;}#sk-container-id-28 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-28 div.sk-item::before, #sk-container-id-28 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-28 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-28 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-28 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-28 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-28 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-28 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-28 div.sk-label-container {text-align: center;}#sk-container-id-28 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-28 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-28\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(C=0.65, random_state=12)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" checked><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=0.65, random_state=12)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(C=0.65, random_state=12)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Linear Support Vector Machine model\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC(C=0.65, random_state=12)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score: 0.18896\n",
      "Linear Support Vector Machine score: 0.16488\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the models\n",
    "print(f\"Logistic Regression score: {logreg.score(X_test, y_test)}\")\n",
    "print(f\"Linear Support Vector Machine score: {svm.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "Run prediction functions such as `predict_proba` and `decision_function` and explain how the values generated by each are calculated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `predict_proba`: uses the softmax function to return a vector of the predicted probabilities for each class\n",
    "\n",
    "- `decision_function`: calculates confidence scores for which classification should be predicted to each data, if a classification score is >0 it should be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression predictions:\n",
      "[[0.22862668 0.27536801 0.49600531]]\n",
      "Linear Support Vector Machine predictions:\n",
      "[[-0.47640029 -0.43062229 -0.04093294]]\n"
     ]
    }
   ],
   "source": [
    "pred_logreg = logreg.predict_proba(X_test[0])\n",
    "pred_svm = svm.decision_function(X_test[0])\n",
    "\n",
    "# Printing the classifications\n",
    "print(f\"Logistic Regression predictions:\\n{pred_logreg}\")\n",
    "print(f\"Linear Support Vector Machine predictions:\\n{pred_svm}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
